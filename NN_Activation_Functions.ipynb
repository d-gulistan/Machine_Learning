{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Activation Functions in Keras and PyTorch</h2>\n",
    "\n",
    "This notebook provides a summary of the activation functions supported by Keras (2.4.0) and PyTorch (1.7.0).\n",
    "\n",
    "Activation functions constitute a crucial component of deep learning. Activation functions are mathematical equations that determine the output of a neural network, the accuracy of the output as well as the computational efficiency of training a model. They serve as a mathematical “gate” in between the input feeding the current neuron and its output going to the next layer.\n",
    "\n",
    "The function is attached to each neuron in the network, and determines whether it should be activated or not. They also help normalize the output of each neuron to a range between 1 and 0 or between -1 and 1. When building a model and training a neural network, the selection of the most approporiate activation functions is crucial. We can achieve better results if we experiment with different activation functions for different problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3 main types of activation functions:\n",
    "1. Binary Step Functions\n",
    "2. Linear Activation Functions\n",
    "3. Non-Linear Activation Functions\n",
    "\n",
    "In this notebook I will only cover Non-Linear Activation Functions for deep neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>ReLU</strong> (= Rectified Linear Unit)\n",
    "\n",
    "- Most commonly used activation function. Many libraries and hardware accelerators provide optimizations specific to ReLU.\n",
    "- ReLU is the best choice when speed is a priority. It is computationally efficient as it allows the network to converge quickly.\n",
    "- The gradient of the ReLU function is 0 when its input is negative.\n",
    "- The output will always be positive if it is used in the output layer.\n",
    "- The function suffers from dying ReLUs: When the weighted sum of the neuron's inputs approach 0 or are negative, the gradient of the function becomes 0, and the network can no longer perform backpropagation or learn. Some neurons die during training, and they only output 0. With a large learning rate, half of the neurons may die.\n",
    "- Dead neurons may eventually come back to life when the Gradient Descent tweaks it in a way that the weighted sum of their inputs turns positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras example 1\n",
    "tf.keras.activations.relu(x, alpha=0.0, max_value=None, threshold=0)\n",
    "\n",
    "# Keras example 2\n",
    "model.add(layers.Dense(32, activation=’relu’))\n",
    "\n",
    "# PyTorch example\n",
    "m = nn.ReLU()\n",
    "input = torch.randn(2)\n",
    "output = m(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Leaky ReLU</strong>\n",
    "\n",
    "- This function is a variation of ReLU, and provides a solution for the dead neurons problem with ReLU.\n",
    "- It has a small positive slope (typically 0.01) in the negative area (z < 0), so it enables backpropagation, even for negative input values. The small slope ensures that the leaky ReLUs never die.\n",
    "- It outperforms the strict ReLU activation function.\n",
    "- This function is a good option if runtime latency is a priority."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras example\n",
    "tf.keras.layers.LeakyReLU(alpha=0.3)\n",
    "\n",
    "# PyTorch example\n",
    "m = nn.LeakyReLU(negative_slope=0.01, inplace=False)\n",
    "input = torch.randn(2)\n",
    "output = m(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>PReLU</strong> (= Parametric Leaky ReLU)\n",
    "\n",
    "- This function allows the negative slope to be learned. It provides the slope of the negative part of the function as an argument.\n",
    "- A disadvantage is that it may perform differently for different problems.\n",
    "\n",
    "<strong>RReLU</strong> (= Randomized Leaky ReLU)\n",
    "\n",
    "- If we have spare time and additional computing power, we can use cross-validation to evaluate PReLU and RReLU. RReLU may be evaluated if the network is overfitting, PReLU may be evaluated if we work with a large training set.\n",
    "- There is no official implementation of RReLU in Keras 2.4.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras example for PReLU\n",
    "tf.keras.layers.PReLU(\n",
    "    alpha_initializer=\"zeros\",\n",
    "    alpha_regularizer=None,\n",
    "    alpha_constraint=None,\n",
    "    shared_axes=None)\n",
    "\n",
    "# PyTorch example for PReLU\n",
    "torch.nn.PReLU(num_parameters=1, init=0.25)\n",
    "\n",
    "# PyTorch example for RReLU\n",
    "torch.nn.RReLU(lower=0.125, upper=0.333, inplace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Thresholded ReLU</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras example\n",
    "tf.keras.layers.ThresholdedReLU(theta=1.0)\n",
    "\n",
    "# PyTorch example\n",
    "torch.nn.Threshold(threshold, value, inplace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>ELU</strong> (= Exponential Linear Unit)\n",
    "\n",
    "- The convergence rate of ELUs is faster during training than a ReLU network, but they are slower to compute than ReLUs and its variants owing to the exponential function.\n",
    "- ELUs saturate to a negative value when the argument gets smaller (z < 0). Mean activations that are closer to 0 enable faster learning, as they bring the gradient closer to the natural gradient.\n",
    "- This function diminishes the vanishing gradient effect, and does not produce dead neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras example\n",
    "tf.keras.activations.elu(x, alpha=1.0)\n",
    "\n",
    "# PyTorch example\n",
    "torch.nn.ELU(alpha=1.0, inplace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>SELU</strong> (= Scaled ELU)\n",
    "\n",
    "- This function significantly outperforms other activation functions for deep neural networks. It may also improve performance in CNNs.\n",
    "- If we build a NN only with a stack of dense layers, and if all hidden layers use the SELU activation function, the network will self-normalize: the output of each layer will preserve a mean of 0 and STD of 1 during training. This solves the vanishing gradient problem.\n",
    "- Requirements for self-normalization:\n",
    "\t- The network’s architecture must be Sequential.\n",
    "    - The input features must be standardized (mean 0 and STD 1).\n",
    "\t- Every hidden layer’s weight must be initialized with kernel_initializer=”lecun_normal”.\n",
    "\t- It needs to be used together with the dropout variant tf.keras.layers.AlphaDropout (not the regular dropout).\n",
    "- If the network’s architecture does not allow for self-normalization, ELU may perform better than SELU. This happens because SELU is not smooth at z = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras example 1\n",
    "tf.keras.activations.selu(x)\n",
    "\n",
    "# Keras example 2\n",
    "model.add(layers.Dense(32, activation=’selu’, kernel_initializer='lecun_normal'))\n",
    "\n",
    "# PyTorch example\n",
    "torch.nn.SELU(inplace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Sigmoid</strong> (or <strong>Logistic</strong>)\n",
    "\n",
    "- Sigmoid is equivalent to a 2-element Softmax, where the second element is assumed to be 0.\n",
    "- This functions has a smooth gradient, which prevents “jumps” in output values.\n",
    "- The output values fall between 0 and 1, which normalizies the output of each neuron, and provides clear predictions.\n",
    "- For small values (<-5) Sigmoid returns a value close to 0, and for large values (>5) the result of the function is close to 1.\n",
    "- Disadvantages of the function: vanishing gradient, outputs are not zero-centered, and it is computationally expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras example\n",
    "tf.keras.activations.sigmoid(x)\n",
    "\n",
    "# PyTorch example\n",
    "torch.nn.Sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>TanH (= Hyperbolic Tangent)</strong>\n",
    "\n",
    "- This is an S-shaped, continuous, and differentiable function.\n",
    "- It is very similar to Sigmoid, but is zero-centered.\n",
    "- TanH scales the labels to the range -1 to 1.\n",
    "- This range makes each layer’s output centered around 0 at the beginning of training, which speeds up convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras example\n",
    "tf.keras.activations.tanh(x)\n",
    "\n",
    "# PyTorch example\n",
    "torch.nn.Tanh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Softmax</strong>\n",
    "\n",
    "- Softmax converts a real vector to a vector of categorical probabilities.\n",
    "- Softmax is often used as the activation function for the output layer of a classification network, because the result could be interpreted as a probability distribution.\n",
    "- The elements of the output vector are in range (0, 1) and sum to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras example\n",
    "tf.keras.activations.softmax(x, axis=-1)\n",
    "\n",
    "# PyTorch example\n",
    "torch.nn.Softmax(dim=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Softplus</strong>\n",
    "\n",
    "- This is a smooth variant of ReLU.\n",
    "- It is close to 0 when z < 0, and close to z when z > 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras example\n",
    "tf.keras.activations.softplus(x)\n",
    "\n",
    "# PyTorch example\n",
    "torch.nn.Softplus(beta=1, threshold=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Softsign</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras example\n",
    "tf.keras.activations.softsign(x)\n",
    "\n",
    "# PyTorch example\n",
    "torch.nn.Softsign"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Additional Activation Function supported in Keras (2.4.0)</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exponential\n",
    "tf.keras.activations.exponential(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Additional Activation Functions supported in PyTorch (1.7.0)</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU6\n",
    "torch.nn.ReLU6(inplace=False)\n",
    "\n",
    "# CELU\n",
    "torch.nn.CELU(alpha=1.0, inplace=False)\n",
    "\n",
    "# GELU\n",
    "torch.nn.GELU\n",
    "\n",
    "# SiLU\n",
    "torch.nn.SiLU(inplace=False)\n",
    "\n",
    "# Softmin\n",
    "torch.nn.Softmin(dim=None)\n",
    "\n",
    "# Softmax2d\n",
    "torch.nn.Softmax2d\n",
    "\n",
    "# Softshrink\n",
    "torch.nn.Softshrink(lambd=0.5)\n",
    "\n",
    "# Hardshrink\n",
    "torch.nn.Hardshrink(lambd=0.5)\n",
    "\n",
    "# Hardsigmoid\n",
    "torch.nn.Hardsigmoid(inplace=False)\n",
    "\n",
    "# Hardtanh\n",
    "torch.nn.Hardtanh(min_val=-1.0, max_val=1.0, inplace=False, min_value=None, max_value=None)\n",
    "\n",
    "# Hardswish\n",
    "torch.nn.Hardswish(inplace=False)\n",
    "\n",
    "# LogSigmoid\n",
    "torch.nn.LogSigmoid\n",
    "\n",
    "# LogSoftmax\n",
    "torch.nn.LogSoftmax(dim=None)\n",
    "\n",
    "# AdaptiveLogSoftmaxWithLoss\n",
    "torch.nn.AdaptiveLogSoftmaxWithLoss(\n",
    "    in_features,\n",
    "    n_classes,\n",
    "    cutoffs,\n",
    "    div_value=4.0,\n",
    "    head_bias=False)\n",
    "\n",
    "# MultiheadAttention\n",
    "torch.nn.MultiheadAttention(\n",
    "    embed_dim,\n",
    "    num_heads,\n",
    "    dropout=0.0,\n",
    "    bias=True,\n",
    "    add_bias_kv=False,\n",
    "    add_zero_attn=False,\n",
    "    kdim=None,\n",
    "    vdim=None)\n",
    "\n",
    "# Tanhshrink\n",
    "torch.nn.Tanhshrink"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a summary, the question arises:\n",
    "\n",
    "<strong>Which activation functions are best to use for hidden layers in a deep neural network?</strong>\n",
    "\n",
    "SELU > ELU > Leaky ReLU (RReLU, PReLU) > ReLU > TanH > Sigmoid"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
